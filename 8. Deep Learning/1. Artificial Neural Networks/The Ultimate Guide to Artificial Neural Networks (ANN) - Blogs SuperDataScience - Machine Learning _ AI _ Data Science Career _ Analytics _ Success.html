<!DOCTYPE html>
<!-- saved from url=(0091)https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" type="text/css" class="__meteor-css__" href="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/13.css">

	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="theme-color" content="#228ae6">

	<link rel="shortcut icon" type="image/png" href="https://www.superdatascience.com/favicon.png">
	<link rel="apple-touch-icon" href="https://www.superdatascience.com/apple-touch-icon-precomposed.png">
	<link rel="manifest" href="https://www.superdatascience.com/manifest.json">

	<title>The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning | AI | Data Science Career | Analytics | Success</title>
	<script async="" src="https://connect.facebook.net/en_US/fbevents.js"></script><script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script type="text/javascript" async="" src="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/f.txt"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-KQQW644"></script><script type="text/javascript" async="" src="https://cdn.segment.com/analytics.js/v1/kTzvFMJQI69rlqzdptxXDvueu4k79bWR/analytics.min.js"></script><script> window.prerenderReady = false; </script> <!-- Tells pre-render not to cache yet. -->

	<!-- Google Tag Manager -->
	<script type="text/javascript" data-cookiecategory="targeting">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KQQW644');</script>

	<!-- Segment Tracking -->
	<script>!function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.src="https://cdn.segment.com/analytics.js/v1/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.13.1";analytics.load("kTzvFMJQI69rlqzdptxXDvueu4k79bWR");}}(); </script>

	<link href="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/icon" rel="stylesheet">

	<!-- JWPlayer Script -->
	<script src="https://cdn.jwplayer.com/libraries/E37GvLkJ.js" async="async"></script>

	<!-- OptinMonster Script -->
	<script type="text/javascript" data-cookiecategory="functionality" src="https://a.optmnstr.com/app/js/api.min.js" data-account="51992" data-user="45908" async=""></script>

	<!-- Cookie Script -->
	<script type="text/javascript" charset="UTF-8" src="https://ca-eu.cookie-script.com/s/07a10c13bde49c13377e33a3c50fa67b.js"></script>

<style type="text/css">.auth-terms{position:absolute;bottom:30px;right:30px;color:#fff}.auth-terms a{color:hsla(0,0%,100%,.5);font-size:15px;margin:0 8px;text-decoration:none;transition:color .2s ease-in-out}.auth-terms a:last-child{margin-right:0}.auth-terms a:hover{color:#fff}</style><style type="text/css">.modal-card{animation:fade-up .5s ease;background-color:#fff;border-color:rgba(0,0,0,.5);border-radius:4px;box-shadow:2px 2px 10px rgba(0,0,0,.3);font-family:Roboto,sans-serif;margin:60px auto;max-width:520px;min-width:320px;outline:none!important;overflow:auto;padding:15px;position:relative;width:100%}.modal-close{border:none;background-color:transparent;cursor:pointer;outline:none;position:absolute;right:10px;top:10px}.modal-close svg{color:#c4ced3;width:15px}.modal-close svg:hover{color:#5f6c80}.modal-card h2{margin-top:0}.modal-card label{display:block;font-weight:500;margin-bottom:10px}.modal-card label input:not([type=checkbox]),.modal-card label textarea{display:block;width:100%;padding:0 5px}.modal-card label input[type=checkbox]{margin-right:5px;vertical-align:top}</style><style type="text/css">.contenteditable-container{display:block;max-width:100%}.contenteditable-editor{display:block;width:100%;border:1px dashed #aaa;min-height:100px;margin-bottom:10px;outline:none;padding:5px;cursor:text;word-break:break-word}.contenteditable-editor:focus{border-color:#333}.contenteditable-editor:empty:before{color:#757575;content:attr(placeholder);display:block;font-size:14px;font-weight:400}.contenteditable-editor img{max-width:100%}.contenteditable-actions{margin-bottom:10px;margin-top:5px}.contenteditable-actions span{display:inline-block;background-color:#d3d3d3;margin:0 3px 3px 0;font-size:13px;cursor:pointer;width:28px;text-align:center}.contenteditable-actions span svg{height:25px;vertical-align:top;padding:5px}.contenteditable-source{display:block;width:100%;max-width:100%;border:1px dashed #aaa;min-height:100px;margin-bottom:10px;outline:none}.contenteditable-source:focus{border-color:#333}</style><style type="text/css">.list-container{margin-bottom:10px}.list-container label{margin-bottom:0}</style><link type="text/css" rel="stylesheet" href="chrome-extension://ckhifbinlmakgeidlbbmplikmcofaedf/mm.css"><script src="https://googleads.g.doubleclick.net/pagead/viewthroughconversion/837004861/?random=1623201952637&amp;cv=9&amp;fst=1623201952637&amp;num=1&amp;guid=ON&amp;resp=GooglemKTybQhCsO&amp;eid=2505059650&amp;u_h=1441&amp;u_w=2560&amp;u_ah=1401&amp;u_aw=2560&amp;u_cd=24&amp;u_his=1&amp;u_tz=480&amp;u_java=false&amp;u_nplug=3&amp;u_nmime=4&amp;gtm=2wg621&amp;sendb=1&amp;ig=1&amp;frm=0&amp;url=https%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann&amp;tiba=SuperDataScience&amp;hn=www.googleadservices.com&amp;async=1&amp;rfmt=3&amp;fmt=4"></script><style type="text/css">@-webkit-keyframes omBounce{0%,20%,50%,80%,to{-webkit-transform:translateY(0);transform:translateY(0)}40%{-webkit-transform:translateY(-30px);transform:translateY(-30px)}60%{-webkit-transform:translateY(-15px);transform:translateY(-15px)}}@keyframes omBounce{0%,20%,50%,80%,to{-webkit-transform:translateY(0);transform:translateY(0)}40%{-webkit-transform:translateY(-30px);transform:translateY(-30px)}60%{-webkit-transform:translateY(-15px);transform:translateY(-15px)}}.om-animation-bounce{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:omBounce;animation-name:omBounce}@-webkit-keyframes omBounceIn{0%{opacity:0;-webkit-transform:scale(.3);transform:scale(.3)}50%{opacity:1;-webkit-transform:scale(1.05);transform:scale(1.05)}70%{-webkit-transform:scale(.9);transform:scale(.9)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@keyframes omBounceIn{0%{opacity:0;-webkit-transform:scale(.3);transform:scale(.3)}50%{opacity:1;-webkit-transform:scale(1.05);transform:scale(1.05)}70%{-webkit-transform:scale(.9);transform:scale(.9)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}.om-animation-bounce-in{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omBounceIn;animation-name:omBounceIn}@-webkit-keyframes omBounceInDown{0%{opacity:0;-webkit-transform:translateY(-2000px);transform:translateY(-2000px)}60%{opacity:1;-webkit-transform:translateY(30px);transform:translateY(30px)}80%{-webkit-transform:translateY(-10px);transform:translateY(-10px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes omBounceInDown{0%{opacity:0;-webkit-transform:translateY(-2000px);transform:translateY(-2000px)}60%{opacity:1;-webkit-transform:translateY(30px);transform:translateY(30px)}80%{-webkit-transform:translateY(-10px);transform:translateY(-10px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}.om-animation-bounce-in-down{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omBounceInDown;animation-name:omBounceInDown}@-webkit-keyframes omBounceInLeft{0%{opacity:0;-webkit-transform:translateX(-2000px);transform:translateX(-2000px)}60%{opacity:1;-webkit-transform:translateX(30px);transform:translateX(30px)}80%{-webkit-transform:translateX(-10px);transform:translateX(-10px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes omBounceInLeft{0%{opacity:0;-webkit-transform:translateX(-2000px);transform:translateX(-2000px)}60%{opacity:1;-webkit-transform:translateX(30px);transform:translateX(30px)}80%{-webkit-transform:translateX(-10px);transform:translateX(-10px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}.om-animation-bounce-in-left{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omBounceInLeft;animation-name:omBounceInLeft}@-webkit-keyframes omBounceInRight{0%{opacity:0;-webkit-transform:translateX(2000px);transform:translateX(2000px)}60%{opacity:1;-webkit-transform:translateX(-30px);transform:translateX(-30px)}80%{-webkit-transform:translateX(10px);transform:translateX(10px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes omBounceInRight{0%{opacity:0;-webkit-transform:translateX(2000px);transform:translateX(2000px)}60%{opacity:1;-webkit-transform:translateX(-30px);transform:translateX(-30px)}80%{-webkit-transform:translateX(10px);transform:translateX(10px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}.om-animation-bounce-in-right{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omBounceInRight;animation-name:omBounceInRight}@-webkit-keyframes omBounceInUp{0%{opacity:0;-webkit-transform:translateY(2000px);transform:translateY(2000px)}60%{opacity:1;-webkit-transform:translateY(-30px);transform:translateY(-30px)}80%{-webkit-transform:translateY(10px);transform:translateY(10px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes omBounceInUp{0%{opacity:0;-webkit-transform:translateY(2000px);transform:translateY(2000px)}60%{opacity:1;-webkit-transform:translateY(-30px);transform:translateY(-30px)}80%{-webkit-transform:translateY(10px);transform:translateY(10px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}.om-animation-bounce-in-up{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omBounceInUp;animation-name:omBounceInUp}@-webkit-keyframes omFlash{0%,50%,to{opacity:1}25%,75%{opacity:0}}@keyframes omFlash{0%,50%,to{opacity:1}25%,75%{opacity:0}}.om-animation-flash{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:omFlash;animation-name:omFlash}@-webkit-keyframes omFlip{0%{-webkit-transform:perspective(800px) translateZ(0) rotateY(0) scale(1);transform:perspective(800px) translateZ(0) rotateY(0) scale(1);-webkit-animation-timing-function:ease-out;animation-timing-function:ease-out}40%{-webkit-transform:perspective(800px) translateZ(150px) rotateY(170deg) scale(1);transform:perspective(800px) translateZ(150px) rotateY(170deg) scale(1);-webkit-animation-timing-function:ease-out;animation-timing-function:ease-out}50%{-webkit-transform:perspective(800px) translateZ(150px) rotateY(190deg) scale(1);transform:perspective(800px) translateZ(150px) rotateY(190deg) scale(1);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}80%{-webkit-transform:perspective(800px) translateZ(0) rotateY(1turn) scale(.95);transform:perspective(800px) translateZ(0) rotateY(1turn) scale(.95);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}to{-webkit-transform:perspective(800px) translateZ(0) rotateY(1turn) scale(1);transform:perspective(800px) translateZ(0) rotateY(1turn) scale(1);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}}@keyframes omFlip{0%{-webkit-transform:perspective(800px) translateZ(0) rotateY(0) scale(1);transform:perspective(800px) translateZ(0) rotateY(0) scale(1);-webkit-animation-timing-function:ease-out;animation-timing-function:ease-out}40%{-webkit-transform:perspective(800px) translateZ(150px) rotateY(170deg) scale(1);transform:perspective(800px) translateZ(150px) rotateY(170deg) scale(1);-webkit-animation-timing-function:ease-out;animation-timing-function:ease-out}50%{-webkit-transform:perspective(800px) translateZ(150px) rotateY(190deg) scale(1);transform:perspective(800px) translateZ(150px) rotateY(190deg) scale(1);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}80%{-webkit-transform:perspective(800px) translateZ(0) rotateY(1turn) scale(.95);transform:perspective(800px) translateZ(0) rotateY(1turn) scale(.95);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}to{-webkit-transform:perspective(800px) translateZ(0) rotateY(1turn) scale(1);transform:perspective(800px) translateZ(0) rotateY(1turn) scale(1);-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in}}.om-animation-flip{-webkit-animation-duration:1s;animation-duration:1s;-webkit-backface-visibility:visible;backface-visibility:visible;-webkit-animation-name:omFlip;animation-name:omFlip}@-webkit-keyframes omFlipInX{0%{-webkit-transform:perspective(800px) rotateX(90deg);transform:perspective(800px) rotateX(90deg);opacity:0}40%{-webkit-transform:perspective(800px) rotateX(-10deg);transform:perspective(800px) rotateX(-10deg)}70%{-webkit-transform:perspective(800px) rotateX(10deg);transform:perspective(800px) rotateX(10deg)}to{-webkit-transform:perspective(800px) rotateX(0deg);transform:perspective(800px) rotateX(0deg);opacity:1}}@keyframes omFlipInX{0%{-webkit-transform:perspective(800px) rotateX(90deg);transform:perspective(800px) rotateX(90deg);opacity:0}40%{-webkit-transform:perspective(800px) rotateX(-10deg);transform:perspective(800px) rotateX(-10deg)}70%{-webkit-transform:perspective(800px) rotateX(10deg);transform:perspective(800px) rotateX(10deg)}to{-webkit-transform:perspective(800px) rotateX(0deg);transform:perspective(800px) rotateX(0deg);opacity:1}}.om-animation-flip-down{-webkit-animation-duration:1s;animation-duration:1s;-webkit-backface-visibility:visible;backface-visibility:visible;-webkit-animation-name:omFlipInX;animation-name:omFlipInX}@-webkit-keyframes omFlipInY{0%{-webkit-transform:perspective(800px) rotateY(90deg);transform:perspective(800px) rotateY(90deg);opacity:0}40%{-webkit-transform:perspective(800px) rotateY(-10deg);transform:perspective(800px) rotateY(-10deg)}70%{-webkit-transform:perspective(800px) rotateY(10deg);transform:perspective(800px) rotateY(10deg)}to{-webkit-transform:perspective(800px) rotateY(0deg);transform:perspective(800px) rotateY(0deg);opacity:1}}@keyframes omFlipInY{0%{-webkit-transform:perspective(800px) rotateY(90deg);transform:perspective(800px) rotateY(90deg);opacity:0}40%{-webkit-transform:perspective(800px) rotateY(-10deg);transform:perspective(800px) rotateY(-10deg)}70%{-webkit-transform:perspective(800px) rotateY(10deg);transform:perspective(800px) rotateY(10deg)}to{-webkit-transform:perspective(800px) rotateY(0deg);transform:perspective(800px) rotateY(0deg);opacity:1}}.om-animation-flip-side{-webkit-animation-duration:1s;animation-duration:1s;-webkit-backface-visibility:visible;backface-visibility:visible;-webkit-animation-name:omFlipInY;animation-name:omFlipInY}@-webkit-keyframes omLightSpeedIn{0%{-webkit-transform:translateX(100%) skewX(-30deg);transform:translateX(100%) skewX(-30deg);opacity:0}60%{-webkit-transform:translateX(-20%) skewX(30deg);transform:translateX(-20%) skewX(30deg);opacity:1}80%{-webkit-transform:translateX(0) skewX(-15deg);transform:translateX(0) skewX(-15deg);opacity:1}to{-webkit-transform:translateX(0) skewX(0deg);transform:translateX(0) skewX(0deg);opacity:1}}@keyframes omLightSpeedIn{0%{-webkit-transform:translateX(100%) skewX(-30deg);transform:translateX(100%) skewX(-30deg);opacity:0}60%{-webkit-transform:translateX(-20%) skewX(30deg);transform:translateX(-20%) skewX(30deg);opacity:1}80%{-webkit-transform:translateX(0) skewX(-15deg);transform:translateX(0) skewX(-15deg);opacity:1}to{-webkit-transform:translateX(0) skewX(0deg);transform:translateX(0) skewX(0deg);opacity:1}}.om-animation-light-speed{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omLightSpeedIn;animation-name:omLightSpeedIn;-webkit-animation-timing-function:ease-out;animation-timing-function:ease-out}@-webkit-keyframes omPulse{0%{-webkit-transform:scale(1);transform:scale(1)}50%{-webkit-transform:scale(1.1);transform:scale(1.1)}to{-webkit-transform:scale(1);transform:scale(1)}}@keyframes omPulse{0%{-webkit-transform:scale(1);transform:scale(1)}50%{-webkit-transform:scale(1.1);transform:scale(1.1)}to{-webkit-transform:scale(1);transform:scale(1)}}.om-animation-pulse{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:omPulse;animation-name:omPulse}@-webkit-keyframes omRollIn{0%{opacity:0;-webkit-transform:translateX(-100%) rotate(-120deg);transform:translateX(-100%) rotate(-120deg)}to{opacity:1;-webkit-transform:translateX(0) rotate(0deg);transform:translateX(0) rotate(0deg)}}@keyframes omRollIn{0%{opacity:0;-webkit-transform:translateX(-100%) rotate(-120deg);transform:translateX(-100%) rotate(-120deg)}to{opacity:1;-webkit-transform:translateX(0) rotate(0deg);transform:translateX(0) rotate(0deg)}}.om-animation-roll-in{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRollIn;animation-name:omRollIn}@-webkit-keyframes omRotateIn{0%{-webkit-transform-origin:center center;transform-origin:center center;-webkit-transform:rotate(-200deg);transform:rotate(-200deg);opacity:0}to{-webkit-transform-origin:center center;transform-origin:center center;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}@keyframes omRotateIn{0%{-webkit-transform-origin:center center;transform-origin:center center;-webkit-transform:rotate(-200deg);transform:rotate(-200deg);opacity:0}to{-webkit-transform-origin:center center;transform-origin:center center;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}.om-animation-rotate{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRotateIn;animation-name:omRotateIn}@-webkit-keyframes omRotateInDownLeft{0%{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(-90deg);transform:rotate(-90deg);opacity:0}to{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}@keyframes omRotateInDownLeft{0%{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(-90deg);transform:rotate(-90deg);opacity:0}to{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}.om-animation-rotate-down-left{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRotateInDownLeft;animation-name:omRotateInDownLeft}@-webkit-keyframes omRotateInDownRight{0%{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(90deg);transform:rotate(90deg);opacity:0}to{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}@keyframes omRotateInDownRight{0%{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(90deg);transform:rotate(90deg);opacity:0}to{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}.om-animation-rotate-down-right{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRotateInDownRight;animation-name:omRotateInDownRight}@-webkit-keyframes omRotateInUpLeft{0%{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(90deg);transform:rotate(90deg);opacity:0}to{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}@keyframes omRotateInUpLeft{0%{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(90deg);transform:rotate(90deg);opacity:0}to{-webkit-transform-origin:left bottom;transform-origin:left bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}.om-animation-rotate-up-left{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRotateInUpLeft;animation-name:omRotateInUpLeft}@-webkit-keyframes omRotateInUpRight{0%{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(-90deg);transform:rotate(-90deg);opacity:0}to{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}@keyframes omRotateInUpRight{0%{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(-90deg);transform:rotate(-90deg);opacity:0}to{-webkit-transform-origin:right bottom;transform-origin:right bottom;-webkit-transform:rotate(0);transform:rotate(0);opacity:1}}.om-animation-rotate-up-right{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omRotateInUpRight;animation-name:omRotateInUpRight}@-webkit-keyframes omRubberBand{0%{-webkit-transform:scale(1);transform:scale(1)}30%{-webkit-transform:scaleX(1.25) scaleY(.75);transform:scaleX(1.25) scaleY(.75)}40%{-webkit-transform:scaleX(.75) scaleY(1.25);transform:scaleX(.75) scaleY(1.25)}60%{-webkit-transform:scaleX(1.15) scaleY(.85);transform:scaleX(1.15) scaleY(.85)}to{-webkit-transform:scale(1);transform:scale(1)}}@keyframes omRubberBand{0%{-webkit-transform:scale(1);transform:scale(1)}30%{-webkit-transform:scaleX(1.25) scaleY(.75);transform:scaleX(1.25) scaleY(.75)}40%{-webkit-transform:scaleX(.75) scaleY(1.25);transform:scaleX(.75) scaleY(1.25)}60%{-webkit-transform:scaleX(1.15) scaleY(.85);transform:scaleX(1.15) scaleY(.85)}to{-webkit-transform:scale(1);transform:scale(1)}}.om-animation-rubber-band{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:omRubberBand;animation-name:omRubberBand}@-webkit-keyframes omShake{0%,to{-webkit-transform:translateX(0);transform:translateX(0)}10%,30%,50%,70%,90%{-webkit-transform:translateX(-10px);transform:translateX(-10px)}20%,40%,60%,80%{-webkit-transform:translateX(10px);transform:translateX(10px)}}@keyframes omShake{0%,to{-webkit-transform:translateX(0);transform:translateX(0)}10%,30%,50%,70%,90%{-webkit-transform:translateX(-10px);transform:translateX(-10px)}20%,40%,60%,80%{-webkit-transform:translateX(10px);transform:translateX(10px)}}.om-animation-shake{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:omShake;animation-name:omShake}@-webkit-keyframes omSlideInDown{0%{opacity:0;-webkit-transform:translateY(-2000px);transform:translateY(-2000px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes omSlideInDown{0%{opacity:0;-webkit-transform:translateY(-2000px);transform:translateY(-2000px)}to{-webkit-transform:translateY(0);transform:translateY(0)}}.om-animation-slide-in-down{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omSlideInDown;animation-name:omSlideInDown}@-webkit-keyframes omSlideInLeft{0%{opacity:0;-webkit-transform:translateX(-2000px);transform:translateX(-2000px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes omSlideInLeft{0%{opacity:0;-webkit-transform:translateX(-2000px);transform:translateX(-2000px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}.om-animation-slide-in-left{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omSlideInLeft;animation-name:omSlideInLeft}@-webkit-keyframes omSlideInRight{0%{opacity:0;-webkit-transform:translateX(2000px);transform:translateX(2000px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes omSlideInRight{0%{opacity:0;-webkit-transform:translateX(2000px);transform:translateX(2000px)}to{-webkit-transform:translateX(0);transform:translateX(0)}}.om-animation-slide-in-right{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omSlideInRight;animation-name:omSlideInRight}@-webkit-keyframes omSwing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@keyframes omSwing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}.om-animation-swing{-webkit-animation-duration:1s;animation-duration:1s;-webkit-transform-origin:top center;transform-origin:top center;-webkit-animation-name:omSwing;animation-name:omSwing}@-webkit-keyframes omTada{0%{-webkit-transform:scale(1);transform:scale(1)}10%,20%{-webkit-transform:scale(.9) rotate(-3deg);transform:scale(.9) rotate(-3deg)}30%,50%,70%,90%{-webkit-transform:scale(1.1) rotate(3deg);transform:scale(1.1) rotate(3deg)}40%,60%,80%{-webkit-transform:scale(1.1) rotate(-3deg);transform:scale(1.1) rotate(-3deg)}to{-webkit-transform:scale(1) rotate(0);transform:scale(1) rotate(0)}}@keyframes omTada{0%{-webkit-transform:scale(1);transform:scale(1)}10%,20%{-webkit-transform:scale(.9) rotate(-3deg);transform:scale(.9) rotate(-3deg)}30%,50%,70%,90%{-webkit-transform:scale(1.1) rotate(3deg);transform:scale(1.1) rotate(3deg)}40%,60%,80%{-webkit-transform:scale(1.1) rotate(-3deg);transform:scale(1.1) rotate(-3deg)}to{-webkit-transform:scale(1) rotate(0);transform:scale(1) rotate(0)}}.om-animation-tada{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omTada;animation-name:omTada}@-webkit-keyframes omWobble{0%{-webkit-transform:translateX(0);transform:translateX(0)}15%{-webkit-transform:translateX(-25%) rotate(-5deg);transform:translateX(-25%) rotate(-5deg)}30%{-webkit-transform:translateX(20%) rotate(3deg);transform:translateX(20%) rotate(3deg)}45%{-webkit-transform:translateX(-15%) rotate(-3deg);transform:translateX(-15%) rotate(-3deg)}60%{-webkit-transform:translateX(10%) rotate(2deg);transform:translateX(10%) rotate(2deg)}75%{-webkit-transform:translateX(-5%) rotate(-1deg);transform:translateX(-5%) rotate(-1deg)}to{-webkit-transform:translateX(0);transform:translateX(0)}}@keyframes omWobble{0%{-webkit-transform:translateX(0);transform:translateX(0)}15%{-webkit-transform:translateX(-25%) rotate(-5deg);transform:translateX(-25%) rotate(-5deg)}30%{-webkit-transform:translateX(20%) rotate(3deg);transform:translateX(20%) rotate(3deg)}45%{-webkit-transform:translateX(-15%) rotate(-3deg);transform:translateX(-15%) rotate(-3deg)}60%{-webkit-transform:translateX(10%) rotate(2deg);transform:translateX(10%) rotate(2deg)}75%{-webkit-transform:translateX(-5%) rotate(-1deg);transform:translateX(-5%) rotate(-1deg)}to{-webkit-transform:translateX(0);transform:translateX(0)}}.om-animation-wobble{-webkit-animation-duration:1s;animation-duration:1s;-webkit-animation-name:omWobble;animation-name:omWobble}.om-content-lock{color:transparent!important;text-shadow:rgba(0,0,0,.5) 0 0 10px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;filter:url("data:image/svg+xml;utf9,<svg%20version='1.1'%20xmlns='http://www.w3.org/2000/svg'><filter%20id='blur'><feGaussianBlur%20stdDeviation='10'%20/></filter></svg>#blur");-webkit-filter:blur(10px);-ms-filter:blur(10px);-o-filter:blur(10px);filter:blur(10px)}html.om-mobile-position,html.om-mobile-position body{position:fixed!important}html.om-ios-form,html.om-ios-form body{-webkit-transform:translateZ(0)!important;transform:translateZ(0)!important;-webkit-overflow-scrolling:touch!important;height:100%!important;overflow:auto!important}html.om-position-popup body{overflow:hidden!important}html.om-position-floating-top{transition:padding-top .5s ease!important}html.om-position-floating-bottom{transition:padding-bottom .5s ease!important}html.om-reset-dimensions{height:100%!important;width:100%!important}.om-verification-confirmation{font-family:Lato,Arial,Helvetica,sans-serif;position:fixed;border-radius:10px;bottom:20px;left:20px;padding:10px 20px;opacity:0;transition:opacity .3s ease-in;background:#85bf31;color:#fff;font-size:18px;font-weight:700;z-index:9999}</style><meta name="description" content="Welcome to the first step of your Deep Learning adventure. First up, Artificial Neural Networks. Sit back, relax, buckle up and get started with Artificial Neural Networks!" dochead="1"><meta property="og:title" content="The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning | AI | Data Science Career | Analytics | Success" dochead="1"><meta property="og:type" content="article" dochead="1"><meta property="og:description" content="Welcome to the first step of your Deep Learning adventure. First up, Artificial Neural Networks. Sit back, relax, buckle up and get started with Artificial Neural Networks!" dochead="1"><meta property="og:url" content="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann" dochead="1"><meta property="og:locale" content="en_US" dochead="1"><meta property="og:site_name" content="SuperDataScience" dochead="1"><meta property="twitter:title" content="The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning | AI | Data Science Career | Analytics | Success" dochead="1"><meta property="twitter:site" content="@superdatasci" dochead="1"><meta property="twitter:creator" content="@superdatasci" dochead="1"><meta property="twitter:description" content="Welcome to the first step of your Deep Learning adventure. First up, Artificial Neural Networks. Sit back, relax, buckle up and get started with Artificial Neural Networks!" dochead="1"><meta property="og:image" content="https://sds-platform-private.s3-us-east-2.amazonaws.com/blog-images/uY6QmcgQy3YeS8AgK" dochead="1"><meta property="twitter:image:src" content="https://sds-platform-private.s3-us-east-2.amazonaws.com/blog-images/uY6QmcgQy3YeS8AgK" dochead="1"><link rel="stylesheet" href="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/css" media="all"><script src="https://a.omappapi.com/app/js/webfont/1.5.18/webfont.js" async=""></script></head>
<body>
  <script type="text/javascript">__meteor_runtime_config__ = JSON.parse(decodeURIComponent("%7B%22meteorRelease%22%3A%22METEOR%401.10.2%22%2C%22gitCommitHash%22%3A%225514055aea8769692fee8a85bba7a91b68717e23%22%2C%22meteorEnv%22%3A%7B%22NODE_ENV%22%3A%22production%22%2C%22TEST_METADATA%22%3A%22%7B%7D%22%7D%2C%22PUBLIC_SETTINGS%22%3A%7B%22promotionalCodes%22%3A%5B%7B%22code%22%3A%22datarockstar%22%2C%22roles%22%3A%5B%22book_cds%22%5D%7D%5D%2C%22platformRoles%22%3A%5B%7B%22role%22%3A%22guest%22%2C%22name%22%3A%22Free%22%7D%2C%7B%22role%22%3A%22payer%22%2C%22name%22%3A%22Payers%22%7D%2C%7B%22role%22%3A%22admin%22%2C%22name%22%3A%22Administrators%22%7D%2C%7B%22role%22%3A%22book_cds%22%2C%22name%22%3A%22Book%20CDS%22%7D%2C%7B%22role%22%3A%22mashable_bundle_1%22%2C%22name%22%3A%22Mashable%20Bundle%201%22%7D%5D%7D%2C%22ROOT_URL%22%3A%22https%3A%2F%2Fwww.superdatascience.com%22%2C%22ROOT_URL_PATH_PREFIX%22%3A%22%22%2C%22autoupdate%22%3A%7B%22versions%22%3A%7B%22web.browser%22%3A%7B%22version%22%3A%22bfb0d6b587dce2ba4d2c5eb7ccad3a6ff64d51d8%22%2C%22versionRefreshable%22%3A%22de9961f6796e5f3943e7651cfb74f5c798e85b2a%22%2C%22versionNonRefreshable%22%3A%227b6697a3d46b5299a7969a61f34ecd4661f3ac40%22%7D%2C%22web.browser.legacy%22%3A%7B%22version%22%3A%229ecc4fb10f5a2f4e4f13143b264f1ba24b635213%22%2C%22versionRefreshable%22%3A%22de9961f6796e5f3943e7651cfb74f5c798e85b2a%22%2C%22versionNonRefreshable%22%3A%2240290c2d1612b583d757d7a636fe73281a822546%22%7D%7D%2C%22autoupdateVersion%22%3Anull%2C%22autoupdateVersionRefreshable%22%3Anull%2C%22autoupdateVersionCordova%22%3Anull%2C%22appId%22%3A%22not499zhpyxm.zvlnxu93xyus%22%7D%2C%22appId%22%3A%22not499zhpyxm.zvlnxu93xyus%22%2C%22isModern%22%3Atrue%7D"))</script>

  <script type="text/javascript" src="https://www.superdatascience.com/6c5f330d13c3a388d2b68ce82eb7b88b55fa2845.js?meteor_js_resource=true"></script>



<div id="__blaze-root">
		<nav class="main-unlogged-navbar">
			<div class="all-articles-container block-animation">
				<a href="https://www.superdatascience.com/">
					<img src="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/Lo.svg" alt="SDS Logo">
				</a>
				<a class="create-account-btn" href="https://www.superdatascience.com/signup"><span class="hidden-xs">Create</span> Free Account</a>
			</div>
		</nav>
	

	<main>
		
  
    
    
    
    
		<div class="fixed-navbar">
			<div class="all-articles-container">
				<div class="title">
					<h4>
						<a href="https://www.superdatascience.com/blogs" class="back-to-blogs">Blogs</a>
						<i class="material-icons">keyboard_arrow_right</i>
						The Ultimate Guide to Artificial Neural Networks (ANN)
					</h4>
				</div>
				<div class="share-buttons hidden-xs">
					<h4>Share</h4>
					<div class="share-item facebook-share-btn">
		<a class="fb-xfbml-parse-ignore" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann&amp;quote=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)&amp;src=sdkpreparse"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="facebook-icon" d="M63.5,1.7c-34.6,0-62.6,28-62.6,62.6s28,62.6,62.6,62.6s62.6-28,62.6-62.6S98.1,1.7,63.5,1.7z M76.6,44.4
		c-1,0-2.1,0-3.1,0c-1.4,0-2.8,0-4.2,0.2c-1.8,0.3-3,1.4-3.1,3.2c-0.1,2.9,0,5.8,0,8.8c3.2,0,6.3,0,9.6,0c-0.4,3.5-0.9,6.8-1.3,10.3
		c-2.8,0-5.5,0-8.3,0c0,8.7,0,17.4,0,26c-3.6,0-7.1,0-10.7,0c0-8.6,0-17.3,0-26c-3,0-5.8,0-8.7,0c0-3.5,0-6.9,0-10.3
		c2.8,0,5.7,0,8.6,0c0-0.5,0-0.9,0-1.2c0.1-3.2-0.1-6.4,0.2-9.6c0.7-6.5,5.3-10.8,12-11c3-0.1,5.9,0.1,9,0.2
		C76.6,38.3,76.6,41.3,76.6,44.4z"></path>
	</svg></a>
	</div>

	<div class="share-item twitter-share-btn">
		<a href="https://twitter.com/intent/tweet?text=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)%20-%20SuperDataScience%0Ahttps%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="twitter-icon" d="M63.3,2C28.8,2,0.8,30,0.8,64.5s28,62.5,62.5,62.5s62.5-28,62.5-62.5S97.9,2,63.3,2z M89.7,51.1
		c-1.8,1.3-2.1,2.8-2.3,4.9c-1,13.1-7,23.3-19,29c-10.6,5.1-21.4,4.4-31.7-1.4c-0.2-0.1-0.3-0.2-0.5-0.4c6.2,0.3,11.8-1.1,16.9-5
		c-5.4-0.6-9-3.1-11-8.3c1.7,0,3.3,0,4.8,0c0-0.1,0.1-0.2,0.1-0.3c-5.6-1.9-8.7-5.6-9-11.5c1.7,0.5,3.4,0.9,5,1.4
		c0.1-0.1,0.1-0.2,0.2-0.3c-3.2-2.4-4.9-5.6-5.1-9.6c-0.1-2.1,0.5-4.1,1.6-6.1c4.3,5.1,9.4,8.7,15.7,10.7c2.8,0.9,5.7,1.5,8.4,1.6
		c0.1-1.9,0-3.8,0.4-5.5c2-8.7,13-11.9,19.5-5.7c0.4,0.4,0.9,0.6,1.4,0.4c2.2-0.8,4.3-1.5,6.5-2.3c-0.8,2.5-2.5,4.4-4.7,5.9
		c0,0.1,0.1,0.2,0.1,0.2c2-0.5,4-1.1,6.4-1.7C92.1,48.6,91,50.1,89.7,51.1z"></path>
	</svg></a>
	</div>

	<div class="share-item">
		<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="linkedin-icon" d="M64.3,2C29.8,2,1.8,30,1.8,64.5s28,62.5,62.5,62.5s62.5-28,62.5-62.5S98.9,2,64.3,2z M52.1,85.6
		c-3.3,0-6.7,0-10.2,0c0-11.1,0-22.1,0-33.3c3.4,0,6.7,0,10.2,0C52.1,63.4,52.1,74.4,52.1,85.6z M47,47.6c-3.3,0-6.1-2.8-6.1-6.2
		c0-3.2,2.7-5.8,6-5.8c3.4,0,6,2.6,6,6.1C52.9,44.8,50.2,47.6,47,47.6z M91.1,85.6c-3.5,0-6.9,0-10.3,0c0-0.5,0-0.9,0-1.3
		c0-5.4,0-10.8,0-16.2c0-1.2-0.2-2.4-0.4-3.6c-0.6-2.7-1.9-3.8-4.8-4c-2.6-0.2-4.9,0.9-5.6,3.2c-0.6,1.7-0.8,3.6-0.9,5.4
		c-0.1,5,0,10,0,15c0,0.5,0,0.9,0,1.5c-3.5,0-6.8,0-10.2,0c0-11.1,0-22.2,0-33.3c3.3,0,6.5,0,9.8,0c0,1.4,0,2.8,0,4.1
		c1.4-1.2,2.7-2.6,4.2-3.4c4.3-2.3,8.8-2,13,0.2c3.1,1.7,4.2,4.8,4.7,8c0.3,1.9,0.5,3.9,0.5,5.9c0.1,5.8,0,11.6,0,17.5
		C91.1,84.9,91.1,85.2,91.1,85.6z"></path>
	</svg></a>
	</div>

	<div class="share-item">
		<a href="mailto:?subject=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)%20-%20SuperDataScience&amp;body=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN).%0Ahttps%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
		<polygon class="email-icon" points="43.2,48.4 43.2,53.7 64.5,67 85.7,53.7 85.7,48.4 64.5,61.7 	"></polygon>
		<path class="email-icon" d="M64.5,1.8c-34.6,0-62.6,28-62.6,62.6s28,62.6,62.6,62.6s62.6-28,62.6-62.6S99.1,1.8,64.5,1.8z M91,80.3 c0,2.9-2.4,5.3-5.3,5.3H43.2c-2.9,0-5.3-2.4-5.3-5.3l0-31.9c0-2.9,2.4-5.3,5.3-5.3h42.5c2.9,0,5.3,2.4,5.3,5.3V80.3z"></path>
	</svg></a>
	</div>
				</div>
			</div>
			<div class="fixed-progress-bar">
				<div class="indicator"></div>
			</div>
		</div>

		<div class="article-header" style="background-image: url(https://sds-platform-private.s3-us-east-2.amazonaws.com/blog-images/uY6QmcgQy3YeS8AgK);">
			<div class="valign-wrapper">
				<div class="content-wrapper">
					<p class="read-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" height="20">
		<path d="M11.99,2.319c-5.52,0-9.99,4.48-9.99,10s4.47,10,9.99,10c5.53,0,10.01-4.48,10.01-10S17.52,2.319,11.99,2.319z M12,20.319 c-4.42,0-8-3.58-8-8s3.58-8,8-8s8,3.58,8,8S16.42,20.319,12,20.319z"></path>
		<path d="M12.5,7.319H11v6l5.25,3.15l0.75-1.23l-4.5-2.67V7.319z"></path>
	</svg> 34 minutes reading time</p>
					<p class="tags">
						<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" height="20">
		<path d="M2.001,8.782c0-1.053-0.002-2.107,0.001-3.16C2.004,4.564,2.644,3.92,3.698,3.919c2.126-0.003,4.252,0.003,6.378-0.003 c0.533-0.002,0.971,0.178,1.349,0.557c2.23,2.239,4.469,4.469,6.7,6.706c0.765,0.767,0.766,1.656,0.001,2.422 c-2.156,2.162-4.315,4.322-6.478,6.478c-0.706,0.704-1.612,0.726-2.317,0.027c-2.291-2.274-4.568-4.561-6.85-6.844 C2.14,12.921,1.999,12.505,2,12.029C2.003,10.947,2.001,9.864,2.001,8.782z M5.657,9.137C6.521,9.134,7.219,8.433,7.22,7.568 c0-0.865-0.697-1.567-1.561-1.57C4.781,5.994,4.075,6.703,4.081,7.581C4.087,8.445,4.791,9.139,5.657,9.137z"></path>
		<path d="M12.879,3.952c0.619,0,1.219-0.05,1.804,0.021c0.298,0.036,0.626,0.231,0.846,0.449c2.283,2.255,4.55,4.525,6.813,6.8 c0.709,0.713,0.724,1.603,0.017,2.316c-2.191,2.21-4.393,4.409-6.602,6.601c-0.635,0.631-1.528,0.626-2.19,0.009 c0.076-0.08,0.149-0.163,0.228-0.242c1.794-1.8,3.588-3.601,5.383-5.401c1.209-1.212,1.208-3.018,0.002-4.224 c-2.037-2.036-4.07-4.076-6.104-6.115C13.012,4.102,12.952,4.032,12.879,3.952z"></path>
	</svg>
						 <span>Artificial Intelligence</span> 
					</p>
					<h2>The Ultimate Guide to Artificial Neural Networks (ANN) </h2>
					<div class="author">
						
							<div class="thumbnail-container image" style="background-image: url(https://sds-platform-public.s3-us-east-2.amazonaws.com/users/77Rsq4n6ESZFEb4b9/profile-image/Y8vtwyP3pakDgquxw)"> </div>
							<div class="information">
								<p>Published by <a href="https://www.superdatascience.com/profile/77Rsq4n6ESZFEb4b9"><strong>SuperDataScience  Team</strong></a></p>
								<p>Saturday Sep 01, 2018</p>
							</div>
						
					</div>
				</div>
			</div>
		</div>

		<div class="article-view-container blog-view-container">
			<div class="article-container">
				<br>
				<div class="block-animation">
					<div><img src="./The Ultimate Guide to Artificial Neural Networks (ANN) - Blogs SuperDataScience - Machine Learning _ AI _ Data Science Career _ Analytics _ Success_files/45.png"><br></div><div><br></div>Welcome... To the first step of your Deep Learning adventure. First up, Artificial Neural Networks. Sit back, relax, buckle up and get started with Artificial Neural Networks!<div><br></div><div>(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-module-1">For the full PPT of Artificial Neural Networks (ANN) Click Here</a>)<br><hr><div><b>Artificial Neural Networks - Plan of Attack</b></div><div>(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-plan-of-attack">For the PPT of this lecture Click Here</a>)</div><div><br></div><div>To help you overcome the complexities inherent in&nbsp;<b>Neural Networking</b>,&nbsp;<i>SuperDataScience</i>&nbsp;has developed a seven-stage Plan of Attack, which is hopefully not a precursor to what our creations do when sentience awakens within them.</div><div><br></div><div><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_2.png"><br></div><div><br></div><div><b>The Neuron</b></div><div>Building from up from the foundation of the Neural Network we will first examine the&nbsp;<b>Neuron</b>; how it works and what it looks like. It is the centerpiece of the Neural Network. For point of comparison, there will be some examination of the human brain; how that works and why we want to replicate it.</div><div><br></div><div><b>The Activation Function</b></div><div>The next stage will cover&nbsp;The <b>Activation Function</b>. This is the process applied to data within the neuron. We will be exploring which are most commonly used and understanding which is most appropriate for your Neural Network.</div><div><br></div><div><b>Practical Application</b></div><div>Then we get into some deep learning on the machinations of the Neural Network. We will follow one in action to see what we are striving towards. But instead of the T2 slicing open his flesh to reveal the robot skeleton beneath, we’ll be looking at&nbsp;<b>how a Neural Network can predict housing prices</b>. Not as dramatic but potentially just as upsetting.</div><div><br></div><div><b>How Neural Networks Learn</b></div><div>The next three tutorials will focus on what makes Neural Networks so fascinating; how they learn. We will be going into deep learning with the&nbsp;<b>Gradient Descent</b>&nbsp;method. Then we will move on to its refined sibling, the&nbsp;<b>Stochastic Gradient Descent</b>&nbsp;method.</div><div><br></div><div>If by this point fiery flashes of Judgement Day have not interrupted your thinking completely, we will have a summary section that covers&nbsp;<b>Backpropagation</b>&nbsp;and how to compile a set of instructions for your Neural Network.</div><hr><div><b>The Neuron</b></div><div>(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-the-neuron">For the PPT of this lecture Click Here</a>)</div><div><br></div><div>In this deep learning tutorial we are going to examine the Neuron in Neural Networking. Briefly, we will cover:</div><div><br></div><div><ul><li>What it is<br></li><li>What it does<br></li><li>Where it fits in the Neural Network<br></li><li>Why it is important<br></li></ul><div><br></div></div><div>The neuron that forms the basis of all Neural Networks is an imitation of what has been observed in the human brain.</div><div><br></div><div style="text-align: center;"><span id="selectionBoundary_1546103189678_12029930633370234" class="rangySelectionBoundary" style="line-height: 0; display: none;">﻿</span><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_3.png"><br></div><div style="text-align: center;"><br></div><div style="text-align: left;">This odd pink critter is just one of the thousands swimming around inside our brains.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Its eyeless head is the neuron. It is connected to other neurons by those tentacles around it called dendrites and by the tails, which are called axons. Through these flow the electrical signals that form our perception of the world around us.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Strangely enough, at the moment a signal is passed between an axon and dendrite, the two don’t actually touch.</div><div style="text-align: left;"><br></div><div style="text-align: left;">A gap exists between them. To continue its journey, the signal must act like a stuntman jumping across a deep canyon on a dirtbike. This jump process of the signal passing is called the <b>synapse</b>. For simplicity’s sake, this is the term I will also use when referring to the passing of signals in our Neural Networks.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>How has the biological neuron been reimagined?</b></div><div style="text-align: left;">Here is a diagram expressing the form a neuron takes in a Neural Network.<br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_4.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">The inputs on the left side represent the incoming signals to the main neuron in the middle. In a human neuron, this these would include smell or touch.</div><div style="text-align: left;"><br></div><div style="text-align: left;">In your Neural Network these inputs are <b>independent variables</b>. They travel down the synapses, go through the big grey circle, then emerge the other side as <b>output values</b>. It is a like-for-like process, for the most part.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The main difference between the biological process and its artificial counterpart is the level of control you exert over the input values; the independent variables on the left-hand side.</div><div style="text-align: left;"><br></div><div style="text-align: left;">You cannot decide how badly something stinks, whether a screeching sound pierces your ears, or how slippery your controller gets after losing, yet again, on FIFA 18.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>You can determine what variables will enter your Neural Network</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">&nbsp;It is important to remember; you must either <b>standardize</b> the values of your independent variables or <b>normalize</b> them. These processes keep your variables within a similar range so it is easier for your Neural Network to process them. This is essential for the operational capacity of your Neural Network.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Observations</b></div><div style="text-align: left;">It is equally important to note that each variable does not stand alone. They are together as a singular observation. For example, you may list a person’s height, age, and weight. These are three different descriptors, but they pertain to one individual person.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Now, once these values pass through the main neuron and break on through to the other side, they become <b>output values</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Output values</b></div><div style="text-align: left;">Output values can take different forms. Take a look at this diagram:</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_5.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">They can either be:</div><div style="text-align: left;"><br></div><div style="text-align: left;"><ul><li>continuous (price)<br></li><li>binary (yes or no)<br></li><li>or categorical.<br></li></ul><div><br></div></div><div style="text-align: left;">A categorical output will fan out into multiple variables.</div><div style="text-align: left;"><br></div><div style="text-align: left;">However, just as the input variables are different parts of a whole, the same goes for a categorical output.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Picture it like a bobsled team: multiple entities packed inside one vehicle.</div><div style="text-align: left;"><br></div><div style="text-align: center;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_6.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Singular Observations</b></div><div style="text-align: left;">It is important to remember so you can keep things clear in your mind when working through this; <b>both the inputs on the left and the outputs on the right are single observations</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The neuron is sandwiched between two single rows of data. There may be three input variables and one output. It doesn’t matter. They are two single corresponding rows of data. One for one.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Back to the Stuntman</b></div><div style="text-align: left;">If he’s marauding over the soft pink terrain of the human brain he will eventually reach the canyon we mentioned before. He needs to jump it.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_7.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">Perhaps there is a crowd of beautiful women and a stockpile of booze on the other side. He can hear ZZ Top blasting from unseen speakers.</div><div style="text-align: left;"><br></div><div style="text-align: left;">He needs to get over there. In the brain, he has to take the leap. He would much rather face this dilemma in a Neural Network, where he doesn’t need the bike to reach his nirvana.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Here, he has a tightrope linking him to the promised land.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_8.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">This is the <b>synapse</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Weights</b></div><div style="text-align: left;">Each synapse is assigned a weight.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Just as the tautness of the tightrope is integral to the stuntman’s survival, so is the weight assigned to each synapse for the signal that passes along it.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Weights are a pivotal factor in a Neural Network’s functioning</b>.</div><div style="text-align: left;">Weights are how Neural Networks learn</div><div style="text-align: left;">&nbsp;Based on each weight, the Neural Network decides what information is important, and what isn’t.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The weight determines which signals get passed along or not, or to what extent a signal gets passed along. The weights are what you will adjust through the process of learning. When you are training your Neural Network, not unlike with your body, the work is done with weights.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Later on I will cover <b>Gradient Descent</b> and <b>Backpropagation</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">These are concepts that apply to the alteration of weights, the hows, and whys; what works best. That’s everything to do with what goes into the neuron, what comes out, and by what means.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>What happens inside the neuron?</b></div><div style="text-align: left;">How are input signals altered in the neuron so they come out the other side as output signals? I’m sad to say it’s slightly less adventurous than a tiny stuntman taking risks on his travels to who-knows-where.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>It all comes down to plain old addition</b>.</div><div style="text-align: left;">First, the neuron takes all the weights it has received and adds them all up. Simple.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It then applies an <b>activation function</b> that has already been applied to either the neuron itself, or an entire layer of neurons. I will go into deeper learning on the activation function later on. For now, all you need to know is that this function facilitates whether a signal gets passed on or not.</div><div style="text-align: left;"><br></div><div style="text-align: left;">That signal goes on to the next neuron down the line then the next, so on and so forth. That’s it.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>In conclusion</b></div><div style="text-align: left;">&nbsp;We have covered:</div><div style="text-align: left;"><br></div><div style="text-align: left;"><ul><li>Input Values<br></li><li>Weights<br></li><li>Synapses<br></li><li>The Neuron<br></li><li>The Activation Function<br></li><li>Output Values<br></li></ul><div><br></div></div><div style="text-align: left;">This is the process you will see repeated again and again and again. . . all the way down the line, hundreds or thousands of times depending on the size of your Neural Network and how many neurons are within it.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Additional Reading</b></div><div style="text-align: left;">For deeper learning on Artificial Neural Networks the Neuron you can read a paper titled Efficient BackProp by Yan LeCun et al. (1998). The link is <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">here</a>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Join me next time as I cover the activation function and try to invent another imaginary thrill-seeker to illustrate the processes there. Happy learning.</div><hr><div style="text-align: left;"><b>The Activation Function</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-the-activation-function">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">In this tutorial we are going to examine an important mechanism within the Neural Network: <b>The activation function</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The activation function is something of a mysterious ingredient added to the input ingredients already bubbling in the <b>neuron’s</b> pot.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_9.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">In the last part of the course we examined the neuron, how it works, and why it is important.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The last section and this one are intrinsically linked because the activation function is the process applied to the weighted input value once it enters the neuron.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Values</b></div><div style="text-align: left;">If weighted <b>input values </b>are shampoo, floor polish, and gin, the neuron would be the deep black pot. The activation function is the open flame beneath that congeals the concoction into something new; the <b>output value</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Just as you can adjust the size of the flame and time you wish to spend stirring, there are many options you can process your input values with.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The Threshold Function</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_10.png"><b><br></b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">The first is the simplest. The x-axis represents the weighted sum of inputs. On the y-axis are the values from 0 to 1. If the weighted sum is valued as less than 0, the TF will pass on the value 0. If the value is equal to or more than 0, the TF passes on 1. It is a yes or no, black or white, binary function.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The Sigmoid Function</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_11.png"><b><br></b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Here is the second method. With the Sigmoid, anything valued below 0 drops off and everything above 0 is valued as 1.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Shaplier than its rigid cousin, the Sigmoid’s curvature means it is far better suited to probabilities when applied at the output layer of your NN. If the Threshold tells you the difference between 0 and 1 dollar, the Sigmoid gives you that as well as every cent in between.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The Rectifier Function</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Up next we have one of the most popular functions applied in global Neural Networking today, and with the most medieval name.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_12.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">You can see why so many people love it.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Look at how the red line presses itself against the x-axis before exploding in a fiery arrow at 45 degrees to a palace above the clouds.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Who wouldn’t want a piece of that sweet action? Its spectacle is matched only by its generosity. Every value is welcome at the party. If a weighted value is below 0 it doesn’t just get abandoned, left to float through time and space between stars and pie signs. It is recruited, indoctrinated. It becomes a 0.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Hyperbolic Tangent Function</b></div><div style="text-align: left;">Following an act like that was never going to be easy, but the <b>Hyperbolic Tangent Function (tanh)</b>, gives it a bloody good go.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_13.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">Tanh is the Dante of the activation functions.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_14.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">It is willing to delve deep below the x-axis and its 0 value to the icy pits of the lowest circle, where the value -1 slumbers.</div><div style="text-align: left;"><br></div><div style="text-align: left;">From there it soars on a route similar to that of the Sigmoid, though over a greater span, all the way to blessed 1.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Conclusion</b></div><div style="text-align: left;">Obviously, this has not been an intensely focused piece on the nuts and bolts of activation functions.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The purpose of this deep learning portion has been to familiarize you with these functions as concepts and to show you the differences between one function and the next.</div><div style="text-align: left;"><br></div><div style="text-align: left;">However, it would be remiss of me to exclude those wanting something more involved.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Additional Reading</b></div><div style="text-align: left;">I recommend the paper Deep sparse rectifier neural networks by Xavier Glorot et al. (2011). The link is <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" style="background-color: rgb(255, 255, 255);">here</a>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It is not essential you read through such dense material yet. I am more concerned with you applying these functions rather than understanding them absolutely. Once you feel comfortable with the practical applications, then the information in the link above will make more sense.</div><div style="text-align: left;"><br></div><div style="text-align: left;">That’s all for now. I hope it was all easily digestible. In the next section, I will help you understand how <b>Neural Networks work</b>.</div><hr><div style="text-align: left;"><b>How do Neural Networks Work?</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-how-do-neural-networks-work">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">Having already looked at the neuron and the activation function, in this tutorial the deep learning begins on <b>how Neural Networks work</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If you have forgotten the structural elements or functionality of Neural Networks, you can always scroll back through the previous articles.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Off the Page and Into Practice</b></div><div style="text-align: left;">This will be a step-by-step examination of how Neural Networks can be applied by using a real-world example. Property valuations.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The pinnacle of your adult life. The moment you realize the time falling asleep on the couch while watching South Park reruns is finally coming to an end.</div><div style="text-align: left;"><br></div><div style="text-align: left;">You are getting on the property ladder.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_15.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Choices, choices</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Almost everybody wants to own a house, but I can’t think of anybody who enjoys buying one. Why is that?</div><div style="text-align: left;"><br></div><div style="text-align: left;">Well, it’s time-consuming, boring, and every ill-considered choice can take you a step closer to long-term financial bondage. It’s so stressful. Every option becomes an imperative you must painstakingly consider, and objectively at that. It’s an almost puzzle to solve.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_16.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">Lucky for us we have Neural Networks. They can find answers quicker than we can and without the hair-tugging vacillation we would likely go through on the way.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Remember</b></div><div style="text-align: left;">One thing to remember before we get into this example. <b>In this section, we will not be training the network</b>. Training is a very important part of Neural Networking but don’t stress, we will be looking at this later on when we better understand how Neural Networks learn. This part is all about application, so we will imagine our Neural Network is already trained up, primed and ready to go.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Back to the task at hand</b></div><div style="text-align: left;">As you will remember, <b>we always begin with a layer of input variables</b>. These are different factors assembled in a single row of data, represented below on the left-hand side.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_17.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">In this simple example we have four variables:</div><div style="text-align: left;"><br></div><div style="text-align: left;"><ul><li>Area (feet sq)<br></li><li>Number of bedrooms<br></li><li>Distance to city (miles)<br></li><li>Age of property<br></li></ul><div><br></div></div><div style="text-align: left;">Their values go through the weighted synapses straight over to the output layer. All four will be analyzed, an activation function will be applied, and the results will be produced.</div><div style="text-align: left;"><br></div><div style="text-align: left;">This is comprehensive enough on a basic level. But <b>there is a way to amplify the power of the Neural Network</b> and increase its accuracy by a very simple addition to the system.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Power up</b></div><div style="text-align: left;">You can <b>implement a hidden layer</b> that sits between the input and output layers.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_18.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">From this new cobweb of arrows, representing the synapses, you begin to understand how these factors, in differing combinations, cast a wider net of possibilities. The result is a far more <b>detailed composite picture</b> of what you are looking for.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Let’s go step-by-step</b></div><div style="text-align: left;">We begin with the four variables on the left and the top neuron of the hidden layer in the middle. All four variable all be connected to the neuron by synapses.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>However, not all of the synapses are weighted. They will either have a 0 value or non 0 value</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The former indicates importance while the latter means they will be discarded.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>For instance</b>, the Area and Distance variable may be valued as non 0. Which means they are weighted. This means they matter. The other two variables, Bedrooms and Age, aren’t weighted and so are not considered by that first neuron. Got it? Good.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>You may wonder why that first neuron is only considering two of the four variables</b>.</div><div style="text-align: left;">In this case, it is common on the property market that larger homes become cheaper the further they are from the city. That’s a basic fact. So what this neuron may be doing is looking specifically for properties that are large but are not so far from the city.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Properties that, for their proximity to a metropolis, have anomalous amounts of square footage. The benefits of this are clear. A person may have a large family but works and whose children go to school in the city.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It would be beneficial for everybody to have their own space at home and also not to have to wake up when the cock crows every morning.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>This is speculation</b></div><div style="text-align: left;">We have not yet done deep learning on training Neural Networks. Based on the variables at hand this is an educated guess as to how the neuron is processing these variables.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Once the Distance and Area criteria have been met, the neuron applies an activation function and makes its own calculations. These two variables will then contribute to the price in the final output layer.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>This is where the power of the Neural Network comes from</b>. There are many of these neurons, each making similar calculations with different combinations of these variables. The next neuron down may have weighted synapses for Area, Bedroom and Age.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It may have been trained up in a specific area or city where there is a high number of families but where many of the properties are new. New properties are often more expensive than old ones.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If you have a new property with three or four bedrooms and large square footage, you can see how the neuron has identified the value of such a place, regardless of its distance to the city.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The way these neurons work and interact means the network itself is extremely flexible</b>, allowing it to look for specific things and therefore make a comprehensive search for whatever it is they have been trained to identify.</div><div style="text-align: left;"><br></div><div style="text-align: left;">That was a simple example of a Neural network in action. In the next tutorial, deep learning on <b>how Neural Networks learn</b> will commence.</div><hr><div style="text-align: left;"><b>How do Neural Networks learn?</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-how-do-neural-networks-learn">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">Now we have seen Neural Networks in action it is time to get into deep learning on how they learn.</div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_19.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">There are two fundamentally different approaches to getting the desired result from your programme.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Hardcoding</b></div><div style="text-align: left;">This is where you tell the programme specific rules and outcomes, then guide it throughout the entire process, accounting for every possible option the programme will have to deal with. It is a more involved process with more interaction between the programmer and programme.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The other approach is what we have been studying so far.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Neural Networking</b></div><div style="text-align: left;">With a Network, you create the facility for the programme to understand what it needs to do <b>independently</b>. You provide the inputs, state the desired outputs, and let it work its own way from one to the other.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The difference between these approaches is as follows. Hardcoding is the man driving his car from one point to another, using road signs and a map to navigate his own way to his destination. Neural Networking is a self-driving Tesla.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_20.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Let’s revisit some old ground before we plow into fresh earth</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Here is a basic Neural Network we have seen many times so far in these tutorials.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_21.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">You have the single row of <b>input variables</b> on the left.</div><div style="text-align: left;"><br></div><div style="text-align: left;">The arrows that represent <b>weighted synapses </b>go into the large <b>neuron</b> in the middle. And on the right, you have the <b>output value</b>. This is called a Single-Layer Feedforward Neural Network. As you can see, the output value above is represented as Y. This is the <b>actual value</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">We are going to replace that with Ŷ, which represents the <b>output value</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The difference between Y and Ŷ is at the core of this entire process</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">When input variables go along the synapses and into the neuron, where an activation function is applied, the resulting data is the output value, Ŷ.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>In order for our Network to learn we need to compare the output value with the actual value</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">There will be a difference between the two.</div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><b>The Cost Function</b></div><div style="text-align: left;">Then we apply what is called a <b>cost function</b> (<a href="https://en.wikipedia.org/wiki/Cost_function">Cost function - Wikipedia</a>), which is one half of the squared difference between the output value and actual value. This is just one commonly used cost function. There are many. We will apply this particular one in our calculations.</div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><b>The cost function tells us the error in our prediction</b>.</div><div style="text-align: left;">Our aim is to minimize the cost function. The lower the cost function, the closer Ŷ is to Y, and hence, the closer our output value to our actual value.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>A lower cost function means higher accuracy for our Network</b>.</div><div style="text-align: left;">Once we have our cost function, a recycling process begins. We feed the resulting data back through the entire Neural Network. The <b>weighted synapses</b> connecting the input variables to the neuron <b>are the only thing we have any control over</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">As long as there exists a disparity between Y and Ŷ, we will need to adjust those weights. Once we tweak them a little we run the Network again. A new cost function will be produced, hopefully, smaller than the last.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Rinse and Repeat</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><b>We need to repeat this until we scrub the cost function down to as small a number as possible</b>, as close to 0 as it will go.</div><div style="text-align: left;"><br></div><div style="text-align: left;">When the output value and actual value are almost touching we know we have optimal weights and can therefore proceed to the testing phase, or application phase.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Example</b></div><div style="text-align: left;">Say we have three input values.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><ul><li>Hours of study<br></li><li>Hours of sleep<br></li><li>Result in a mid-semester quiz<br></li></ul><div><br></div></div><div style="text-align: left;">Based on these variables we are trying to calculate the result in an upcoming exam. Let’s say the result of the exam is 93%. That would be our actual value, Y.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_22.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">We feed the variables through the weighted synapses and the neuron to calculate our output value, Ŷ.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Then the cost function is applied</b> and the data goes in reverse through the Neural Network.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If there is a disparity between Y and Ŷ then the weights will be adjusted and the process can begin all over again. Rinse and repeat until the cost function is minimized.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_23.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">In this example that would mean our output value would equal the actual value of the 93% test score.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Remember, the input variables do not change</b>.</div><div style="text-align: left;">It is only the weight of the synapses that alters after the cost function comes into play. To give you an idea of the potential scope of this process we can extend this example. The simple Neural Network cited above could be applied to a single student.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Go Bigger</b></div><div style="text-align: left;">What if you wanted to apply this process to an entire class? You would simply need to duplicate these smaller Networks and repeat the process again.</div><div style="text-align: left;"><br></div><div style="text-align: left;">However, once you do this you will not have a number of smaller networks processing separately side-by-side.&nbsp;</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>They actually merge to form a single, much larger Neural Network</b>.</div><div style="text-align: left;">So when you again go through the process of minimizing the difference between Y and Ŷ for an entire class, the cost function phase at the end will adjust for every student simultaneously.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If you have thirty students the Y / Ŷ comparison will occur thirty times in each smaller network but the cost function will be applied to all of them together.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_24.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">As a result, the weights for every student will be adjusted accordingly, so on and so forth.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Additional Reading</b></div><div style="text-align: left;">For further reading on this process, I will direct you towards an article named A list of cost functions used in neural networks, alongside applications. CrossValidated (2015).</div><div style="text-align: left;"><br></div><div style="text-align: left;">I hope you found something useful in this deep learning article. See you next time.</div><hr><div style="text-align: left;"><b>Gradient Descent</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-gradient-descent">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">This is a <b>continuation</b> of the last deep learning section on <b>how Neural Networks learn</b>. If you recall, we summed up the learning process for Neural Networks by focusing on one particular area.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Reducing the Cost Function</b></div><div style="text-align: left;">The cost function is the difference between the output value produced at the end of the Network and the actual value. The closer these two values, the more accurate our Network, and the happier we are. How do we reduce the cost function?</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Backpropagation</b></div><div style="text-align: left;">This is when you feed the end data back through the Neural Network and then adjust the weighted synapses between the input value and the neuron.</div><div style="text-align: left;"><br></div><div style="text-align: left;">By repeating this cycle and adjusting the weights accordingly, you reduce the cost function.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>How do we adjust the weighted synapses?</b></div><div style="text-align: left;">In the last article, I pointed out that the weights are the only thing within our Network that we can tamper with. Throughout the rest of the process, the Network is essentially operating independently.</div><div style="text-align: left;"><br></div><div style="text-align: left;">This puts all the more emphasis on how and why we alter the weights.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If you make the wrong alteration it would be like having a car with the front axle pointing slightly to the left, and while you are driving you to let go of the steering wheel but keep your foot on the gas.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Sooner or later, you’re going to crash.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_25.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Two ways to adjust the weights</b></div><div style="text-align: left;">The first is the brute-force approach. This is far better suited to a single-layer feet-forward network. Here you take a number of possible weights. On a graph it looks like this:</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_26.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">You can see there are a number of blue dots. As we have already established, we want to lower the cost function i.e. reduce the difference between our output value and actual values.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>We want to eliminate all other weights except for the one right at the bottom of the U-shape</b>, the one closest to 0. The closer to 0, the lower the difference between output value and actual value.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>That is where we find our optimal weight</b>.</div><div style="text-align: left;">You could find your way to the best weight through a simple process of elimination. You could trial every weight through your network and one-by-one get closer and closer to your optimal weight. On a simple level, this would suffice, say if you only had a single weight to optimize. But the larger a network becomes, the number of weights that will emerge means this method is impracticable.</div><div style="text-align: left;"><br></div><div style="text-align: left;">You could have an almost infinite amount of weights to adjust. Therefore a process of elimination would be like looking for the prettiest grain of sand on a beach. Good luck with that. This problem is known as the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" style="background-color: rgb(255, 255, 255);">Curse of Dimensionality</a>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">To put a mind-bending timescale on it, an advanced supercomputer would need a longer time than the universe has existed to find all the optimal weights in a mildly complex Neural Network, when using this process of elimination.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Obviously, that won’t do.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Gradient Descent</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_27.png"><b><br></b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Here you see how <b>gradient descent</b> works in a simple graphic representation.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Instead of going through every weight one at a time, and ticking every wrong weight off as you go, <b>you instead look at the angle of the cost function line</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If the slope is negative, like it is from the highest red dot on the above line, that means you must go downhill from there.</div><div style="text-align: left;"><br></div><div style="text-align: left;">For the sake of remembering the journey downwards, you can picture it like the old Prince of Persia video games.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_28.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">You need to jump across the open space, from ledge to ledge, until you get to the checkpoint at the bottom of the level, which is our 0.</div><div style="text-align: left;"><br></div><div style="text-align: left;">This eliminates a vast number of incorrect weights on the way down. It also reduces the time and effort spent on finding the right weight.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It is a <b>far more efficient method</b>, I’m sure you will agree. That is just an examination of the process in principle.</div><div style="text-align: left;"><br></div><div style="text-align: left;">At the end of the next section, I will provide you with some additional reading and there will be further information once we reach the application phase.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Next, we will go into deep learning on <b>Stochastic Gradient Descent</b>. See you there.</div><hr><div style="text-align: left;"><b>Stochastic Gradient Descent</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-stochastic-gradient-descent">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">This article is the second in a two-parter covering methods by which we can find the optimal weight for our synapses.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Just to remind you, the weights are the only thing we can adjust as we look to bring the difference between our output value and our actual value as close to 0 as possible. This is important because the smaller the difference between those two value, the better our Neural Network is performing.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Gradient Descent</b></div><div style="text-align: left;">You can see the aforementioned method illustrated in the graph below.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_29.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">Gradient Descent is a <b>time-saving method </b>with which you are able to leapfrog large chunks of unusable weights by leaping down and across the U-shape on the graph. This is great if the weights you have amassed fit cleanly into this U-shape you see above. This happens when you have one global minimum, the single lowest point of all you weights. However, depending on the cost function you use, your graph will not always plot out so tidily.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The Difference between Local and Global</b></div><div style="text-align: left;">If you use the aforementioned Gradient Descent method here, you may very well end up with what is called a <b>local minimum</b> of the cost function.</div><div style="text-align: left;"><br></div><div style="text-align: left;">What you want is the <b>global minimum</b>. See where the red dot is sitting? That is the local minimum. It is a low point on the gradient. But not the lowest. That would be the purple dot, further down.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_30.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">If you criss-cross down the way you would in the first graph in the article, you may end up stuck at the second-lowest point and never reach optimal weight, which would be the lowest.</div><div style="text-align: left;"><br></div><div style="text-align: left;">For this reason, another method has been developed for when you have a non-convex shape on your graph.</div><div style="text-align: left;"><br></div><div style="text-align: left;">With this, you won’t get bogged down in the nucks higher up on the line before you get to the bottom.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_31.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">This alternative method is called Stochastic Gradient Descent.&nbsp;</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Stochastic Gradient Descent</b></div><div style="text-align: left;">The difference between Gradient Descent and Stochastic Gradient Descent, aside from the one extra word, lies in how each method adjusts the weights in a Neural Network.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Let’s say we have ten rows of data in our Neural Network.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><ul><li>We plug them in.<br></li><li>We calculate our cost function based on whatever formula we happen to be using.<br></li></ul><div><br></div></div><div style="text-align: left;">Now, with the Gradient Descent method, all the weights for all ten rows of data are <b>adjusted simultaneously</b>. This is good because it means you start with the same weights across the board every time. The weights move like a flock of birds, all together in the same direction, all the time.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>It is a deterministic method</b>.</div><div style="text-align: left;">On the other hand, this can take a little longer than the Stochastic method because with every adjustment, every piece of data has to be loaded up all over again.</div><div style="text-align: left;"><br></div><div style="text-align: left;">With the Stochastic method, each weight is adjusted individually.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>So, we go to the first row</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><ul><li>Run the Neural Network.<br></li><li>Look at the cost function.<br></li><li>Then we adjust the weights.<br></li></ul><div><br></div></div><div style="text-align: left;"><b>Then we go to the second row</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><ul><li>Run the Neural Network.<br></li><li>Look at the cost function.<br></li><li>Adjust the weights.<br></li></ul><div><br></div></div><div style="text-align: left;">Row after row we do this until all ten rows have been run through.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_32.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>The Stochastic method is a lighter algorithm and therefore faster</b> than its all-encompassing cousin.</div><div style="text-align: left;"><br></div><div style="text-align: left;">It also has much higher fluctuations, so on a non-convex graph it is more likely to find the global minimum rather than the local minimum. There is a hybrid of these two approaches called the Mini-Batch method.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Mini-batch method</b></div><div style="text-align: left;">With this, you don’t have to either run one row at a time or every row at once. If you have a hundred rows of data you can do five or ten at a time, then update your weights after every subsection has been run.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_33.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Additional Reading</b></div><div style="text-align: left;">&nbsp;There is some additional reading available for this section. I recommend A Neural Network in 13 Lines of Python (Part 2 - Gradient Descent) by Andrew Trask (2015). The link is <a href="https://iamtrask.github.io/2015/07/27/python-network-part2/">here</a>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">There is also a book called Neural Networks and Deep Learning by Michael Nielsen (2015).</div><div style="text-align: left;"><br></div><div style="text-align: left;">That is the nutshell version of the differences between Gradient Descent and Stochastic Gradient Descent. Our next and final section will cover Backpropagation. See you there.</div><hr><div style="text-align: left;"><b>Backpropagation</b></div><div style="text-align: left;">(<a href="https://www.slideshare.net/KirillEremenko/deep-learning-az-artificial-neural-networks-ann-backpropagation">For the PPT of this lecture Click Here</a>)</div><div style="text-align: left;"><br></div><div style="text-align: left;">Unfortunately friends, this is our final deep learning tutorial.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_34.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;">But fear not, before the tears begin to flow too freely, you can stem the tide with some lovely, soft <a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a>.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Adjusting</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;">Backpropagation is an advanced algorithm, driven by sophisticated mathematics, which <b>allows us to adjust all the weights in our Neural Network</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">This is important because it is through the manipulation of weights that we bring the output value (the value produced by our Neural Network) and the actual value closer together.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>These two values need to be as close as possible</b>.</div><div style="text-align: left;"><br></div><div style="text-align: left;">If they are far apart we use Backpropagation to run the data in reverse through the Neural Network. The weights get adjusted. Consequently, we are brought closer to the actual value.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><img src="https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/45_blog_image_35.png"><br></div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>In a Nutshell</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><b>The key underlying principle</b> of Backpropagation is that the structure of the algorithm allows for large numbers of weights to be adjusted simultaneously.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>This drastically speeds up the process</b> and is a key ingredient as to why Neural Networks are able to function as well as they do.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>Additional Reading</b></div><div style="text-align: left;">I mentioned this in the last post but I will include some additional reading material in case you want to get into some extra deep learning on this process. Neural Networks and Deep Learning by Michael Nielsen (2015) is all you will need to go full Einstein on this subject. Give it a look.</div><div style="text-align: left;"><br></div><div style="text-align: left;">Several times throughout this course I have mentioned the importance of training your Neural Network. I wanted you to see the Networks in action before examining how they operate, I have left this until last.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><b>A step-by-step training guide for your Neural Network</b></div><div style="text-align: left;"><b><br></b></div><div style="text-align: left;"><ol><li>Randomly initialize the weights to small numbers close to 0 (but not 0).<br></li><li>Input the first observation. One feature per input node.<br></li><li>Forward-Propagation. From left to right the neurons are activated and the output value is produced.<br></li><li>Compare output value to actual value. Measure the difference between the two; the generated error.<br></li><li>From right to left the generated error is back-propagated and the weights adjusted accordingly. The learning rate of the Network is dependent on how much you adjust the weights.<br></li><li>Repeat steps 1-5 and either adjust the weights after each observation (Reinforcement learning), or after a batch of observations (Batch learning).<br></li><li>When the whole training set passes through the Neural Network, that makes an epoch. Redo more epochs.<br></li></ol></div><div style="text-align: left;"><br></div><div style="text-align: left;">And that is that. I hope this deep learning experience has been somewhat useful to you. I hope you come back soon.</div><div style="text-align: left;"><br></div><div style="text-align: left;"><a href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn/">Click Here to continue with Convolutional Neural Networks (CNN)</a></div></div>
				</div>

				<div class="article-share-container">
					<h4>Share on</h4>
					<div class="share-item facebook-share-btn">
		<a class="fb-xfbml-parse-ignore" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann&amp;quote=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)&amp;src=sdkpreparse"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="facebook-icon" d="M63.5,1.7c-34.6,0-62.6,28-62.6,62.6s28,62.6,62.6,62.6s62.6-28,62.6-62.6S98.1,1.7,63.5,1.7z M76.6,44.4
		c-1,0-2.1,0-3.1,0c-1.4,0-2.8,0-4.2,0.2c-1.8,0.3-3,1.4-3.1,3.2c-0.1,2.9,0,5.8,0,8.8c3.2,0,6.3,0,9.6,0c-0.4,3.5-0.9,6.8-1.3,10.3
		c-2.8,0-5.5,0-8.3,0c0,8.7,0,17.4,0,26c-3.6,0-7.1,0-10.7,0c0-8.6,0-17.3,0-26c-3,0-5.8,0-8.7,0c0-3.5,0-6.9,0-10.3
		c2.8,0,5.7,0,8.6,0c0-0.5,0-0.9,0-1.2c0.1-3.2-0.1-6.4,0.2-9.6c0.7-6.5,5.3-10.8,12-11c3-0.1,5.9,0.1,9,0.2
		C76.6,38.3,76.6,41.3,76.6,44.4z"></path>
	</svg></a>
	</div>

	<div class="share-item twitter-share-btn">
		<a href="https://twitter.com/intent/tweet?text=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)%20-%20SuperDataScience%0Ahttps%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="twitter-icon" d="M63.3,2C28.8,2,0.8,30,0.8,64.5s28,62.5,62.5,62.5s62.5-28,62.5-62.5S97.9,2,63.3,2z M89.7,51.1
		c-1.8,1.3-2.1,2.8-2.3,4.9c-1,13.1-7,23.3-19,29c-10.6,5.1-21.4,4.4-31.7-1.4c-0.2-0.1-0.3-0.2-0.5-0.4c6.2,0.3,11.8-1.1,16.9-5
		c-5.4-0.6-9-3.1-11-8.3c1.7,0,3.3,0,4.8,0c0-0.1,0.1-0.2,0.1-0.3c-5.6-1.9-8.7-5.6-9-11.5c1.7,0.5,3.4,0.9,5,1.4
		c0.1-0.1,0.1-0.2,0.2-0.3c-3.2-2.4-4.9-5.6-5.1-9.6c-0.1-2.1,0.5-4.1,1.6-6.1c4.3,5.1,9.4,8.7,15.7,10.7c2.8,0.9,5.7,1.5,8.4,1.6
		c0.1-1.9,0-3.8,0.4-5.5c2-8.7,13-11.9,19.5-5.7c0.4,0.4,0.9,0.6,1.4,0.4c2.2-0.8,4.3-1.5,6.5-2.3c-0.8,2.5-2.5,4.4-4.7,5.9
		c0,0.1,0.1,0.2,0.1,0.2c2-0.5,4-1.1,6.4-1.7C92.1,48.6,91,50.1,89.7,51.1z"></path>
	</svg></a>
	</div>

	<div class="share-item">
		<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
	<path class="linkedin-icon" d="M64.3,2C29.8,2,1.8,30,1.8,64.5s28,62.5,62.5,62.5s62.5-28,62.5-62.5S98.9,2,64.3,2z M52.1,85.6
		c-3.3,0-6.7,0-10.2,0c0-11.1,0-22.1,0-33.3c3.4,0,6.7,0,10.2,0C52.1,63.4,52.1,74.4,52.1,85.6z M47,47.6c-3.3,0-6.1-2.8-6.1-6.2
		c0-3.2,2.7-5.8,6-5.8c3.4,0,6,2.6,6,6.1C52.9,44.8,50.2,47.6,47,47.6z M91.1,85.6c-3.5,0-6.9,0-10.3,0c0-0.5,0-0.9,0-1.3
		c0-5.4,0-10.8,0-16.2c0-1.2-0.2-2.4-0.4-3.6c-0.6-2.7-1.9-3.8-4.8-4c-2.6-0.2-4.9,0.9-5.6,3.2c-0.6,1.7-0.8,3.6-0.9,5.4
		c-0.1,5,0,10,0,15c0,0.5,0,0.9,0,1.5c-3.5,0-6.8,0-10.2,0c0-11.1,0-22.2,0-33.3c3.3,0,6.5,0,9.8,0c0,1.4,0,2.8,0,4.1
		c1.4-1.2,2.7-2.6,4.2-3.4c4.3-2.3,8.8-2,13,0.2c3.1,1.7,4.2,4.8,4.7,8c0.3,1.9,0.5,3.9,0.5,5.9c0.1,5.8,0,11.6,0,17.5
		C91.1,84.9,91.1,85.2,91.1,85.6z"></path>
	</svg></a>
	</div>

	<div class="share-item">
		<a href="mailto:?subject=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN)%20-%20SuperDataScience&amp;body=The%20Ultimate%20Guide%20to%20Artificial%20Neural%20Networks%20(ANN).%0Ahttps%3A%2F%2Fwww.superdatascience.com%2Fblogs%2Fthe-ultimate-guide-to-artificial-neural-networks-ann"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 128 128.9" style="enable-background:new 0 0 128 128.9;" width="26">
		<polygon class="email-icon" points="43.2,48.4 43.2,53.7 64.5,67 85.7,53.7 85.7,48.4 64.5,61.7 	"></polygon>
		<path class="email-icon" d="M64.5,1.8c-34.6,0-62.6,28-62.6,62.6s28,62.6,62.6,62.6s62.6-28,62.6-62.6S99.1,1.8,64.5,1.8z M91,80.3 c0,2.9-2.4,5.3-5.3,5.3H43.2c-2.9,0-5.3-2.4-5.3-5.3l0-31.9c0-2.9,2.4-5.3,5.3-5.3h42.5c2.9,0,5.3,2.4,5.3,5.3V80.3z"></path>
	</svg></a>
	</div>
				</div>
				<hr class="article-related-separator">
			</div>

			<div class="text-container">
				<div class="all-articles-container article-view-related-container">
					<h1>Related blogs</h1>

					
						
		<div class="article-item blog-related-item block-animation">
			<div class="image-container">
				<div class="thumbnail-container image" style="background-image: url(https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/B96-Header-Image.jpg)"> </div>
			</div>
			<div class="information">
				<h4>Data Science: The Soft Skills Handbook</h4>
				<p>Here are the best tips we here at SuperDataScience can give for both new data scientists (and for a large portion of experienced ones who maybe slipped under the radar) keen to build their softer side.</p>
			</div>
		</div>
	
					
						
		<div class="article-item blog-related-item block-animation">
			<div class="image-container">
				<div class="thumbnail-container image" style="background-image: url(https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/B95-Header-Image.jpg)"> </div>
			</div>
			<div class="information">
				<h4>9 Tips on How To Be the Best at Remote Working</h4>
				<p>Here at SuperDataScience (SDS) we are advocates for teleworking and are excited to be able to share with you our tips on how we make it work.</p>
			</div>
		</div>
	
					
						
		<div class="article-item blog-related-item block-animation">
			<div class="image-container">
				<div class="thumbnail-container image" style="background-image: url(https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/B94-Header-Image.jpg)"> </div>
			</div>
			<div class="information">
				<h4>Smarter Exercising for the Data Scientist</h4>
				<p>After reading this you’ll be better equipped to know when and how to get that blood flowing to keep you at your A-game. </p>
			</div>
		</div>
	
					
				</div>
			</div>

			<footer class="footer-logged">
		<div class="all-articles-container">
			<div class="social-buttons">
				<a class="social-item" href="https://www.facebook.com/superdatascience">
					<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 127 127" style="enable-background:new 0 0 127 127;" width="26">
	<path d="M69.4,57.5c-5.9,0-11.4,0-17.3,0c0,3.2,0,6.1,0,9.1c0,2.7,0.1,5.3,0,8c-0.1,1.8,0.6,2.3,2.3,2.2c4.8-0.1,9.6,0,14.7,0 c0,16.6,0,32.9,0,49.6c-0.9,0-1.7,0-2.6,0c-17,0-34,0-51,0c-9.6,0-15-5.4-15-15c0-31.9,0-63.8,0-95.7c0-9.8,5.4-15.1,15.2-15.1 c31.8,0,63.7,0,95.5,0c10,0,15.3,5.3,15.3,15.3c0,31.9,0,63.8,0,95.7c0,9.4-5.4,14.9-14.9,14.9c-7.4,0-14.8,0-22.4,0 c0-16.6,0-33.1,0-49.7c5.5,0,11,0,16.9,0c0.8-6.4,1.7-12.6,2.5-19.1c-6.5,0-12.8,0-19.8,0c0.3-5.8,0.3-11.3,1-16.8 c0.4-2.9,3.2-4.2,6-4.4c3.2-0.2,6.3-0.1,9.5-0.2c1,0,2,0,3.1,0c0-5.8,0-11.4,0-17.2c-8.6-0.5-17.3-2.1-25.6,1.5 c-8.7,3.7-12.7,11-13.3,20.1C69.2,46.1,69.4,51.6,69.4,57.5z"></path>
	</svg>
				</a>
				<a class="social-item" href="https://www.linkedin.com/company/superdatascience/">
					<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 126 126" style="enable-background:new 0 0 126 126;" width="26">
		<path d="M126.4,126.4c-8.7,0-17.1,0-25.9,0c0-1,0-1.8,0-2.7c0-12.8,0-25.7,0-38.5c0-2.9-0.2-5.8-0.4-8.7c-1-10.3-7.6-15-17.7-12.8 C76,65.1,72,70.2,71.5,78.4c-0.4,8.3-0.4,16.7-0.4,25c-0.1,7.6,0,15.2,0,22.9c-8.7,0-17.2,0-25.8,0c0-27.9,0-55.7,0-83.6 c8.2,0,16.3,0,24.8,0c0,3.6,0,7.2,0,10.8c0.2,0,0.3,0.1,0.5,0.1c0.9-1.2,1.8-2.5,2.8-3.6c6.5-7.5,15.1-10,24.7-9.6 c6.8,0.3,13.2,1.8,18.5,6.4c5.6,4.9,7.8,11.5,8.9,18.5c0.6,3.9,1,7.8,1,11.7c0.1,15.8,0.1,31.7,0.1,47.5 C126.5,125,126.4,125.6,126.4,126.4z"></path>
		<path d="M28.4,126.4c-8.6,0-17,0-25.6,0c0-27.9,0-55.8,0-83.8c8.5,0,17,0,25.6,0C28.4,70.5,28.4,98.3,28.4,126.4z"></path>
		<path d="M30.6,15.5c0.1,8.1-6.6,15.3-14.3,15.4C7.7,31,0.7,24.2,0.6,15.8C0.4,7.5,7.1,0.7,15.5,0.6C23.8,0.4,30.6,7.1,30.6,15.5z"></path>
	</svg>
				</a>
				<a class="social-item" href="https://twitter.com/superdatasci">
					<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 152 126" style="enable-background:new 0 0 152 126;" width="26">
		<path d="M9,6.9c17.2,19.7,38.3,30.8,64.7,32.6c-0.1-2.1-0.2-3.8-0.3-5.4C72.9,21,79.6,9.3,91,3.8c11.5-5.6,25.1-3.5,35,4.9 c1.1,1,3.4,1.6,4.8,1.2c5.5-1.7,10.9-3.8,16.5-5.8c-2,6.5-6.3,11.3-11.6,16c2.5-0.5,5-0.9,7.5-1.5c2.6-0.7,5.2-1.5,7.8-2.3 c0.2,0.2,0.5,0.5,0.7,0.7c-4,4-7.8,8.3-12,12c-2.3,2-3.2,3.8-3.2,6.8c-0.3,39.8-25.2,74.4-62.7,85.9c-24.2,7.4-47.9,5.1-70.4-7 c-1.2-0.6-2.3-1.4-3.5-2.5c16.3,0.7,31.1-3.3,44.9-13.5c-14.4-1.6-24.1-8.3-29.1-21.5c4.5-0.2,8.6-0.4,12.7-0.6 C13.6,71.1,5.5,61,4.3,45.4c4.6,1.3,8.8,2.4,13.8,3.9C3.6,31.9,1,19.5,9,6.9z"></path>
	</svg>
				</a>
			</div>
			<div class="actions">
				
					<a href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann">Privacy Policy</a>
				
					<a href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann">Terms &amp; Conditions</a>
				
					<a href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann">Careers</a>
				
					<a href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann">Contact us</a>
				
			</div>
			<div class="information">
				<span>© 2021 SuperDataScience</span>
			</div>
		</div>
	</footer>
		</div>

		
	
  
  
	</main>

	<div class="bug-report hidden-xs">
		<i class="material-icons">live_help</i>
	</div>

	<div class="status-message-container">
		
	</div></div>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1652381645053689");fbq("set","agent","tmgoogletagmanager","1652381645053689");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1652381645053689&amp;ev=PageView&amp;noscript=1"></noscript>
<div id="om-oluolbqi52h67o0x85u9-holder"></div><div id="om-nbr5q4bugib6o9s31mlu-holder"></div><div id="om-mbjzy5agtckina9vygds-holder"></div><div id="om-qdqnrzohuomq2ydbgpdt-holder"></div><div id="om-ew1bgg35xrtomvheg8h9-holder"></div><div id="om-ws3w5das1g14dkxqepf0-holder"></div><div id="om-za3fxbsbjbipu8x1lgec-holder"></div><div id="om-iguypajsnjtbvhoxd42g-holder"></div><div id="om-efxl2uotrgtcdanuuj21-holder"></div><div id="om-jio64neycytskbkohsw8-holder"></div><div id="om-r4txoiqsvzixesxuhitw-holder"></div><div id="om-fzp9sdxoehcnzutci6x8-holder"></div><div id="om-xakcz9je0yzfrmraalce-holder"></div><div id="om-jkeljfkf9t5j3watbvzv-holder"></div><div id="om-qg6e96qqxccxpcetzqjf-holder"></div><div id="om-s7ie07cjecqlwcodogs8-holder"></div><div id="om-g8earmy9pgkczjrjj4bn-holder"></div><div id="om-z2o3fkvy2r2qhyezckgb-holder"></div><div id="om-s1yxhphmi9rllrbycos9-holder"></div><div id="om-wlkrstavnni4054rpcge-holder"></div><div id="om-w0sevrvej8tx5qflnraj-holder"></div></body></html>